{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7496965,"sourceType":"datasetVersion","datasetId":4365344}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dynamic Ride Pricing","metadata":{}},{"cell_type":"markdown","source":"### Overview\nA ride-sharing company wants to implement a dynamic pricing strategy to optimize fares based on real-time market conditions. The company only uses ride duration to decide ride fares currently. The company aims to leverage data-driven techniques to analyze historical data and develop a predictive model that can dynamically adjust prices in response to changing factors.\n\n### Dataset\nThe dataset provided for this project contains historical ride data and includes features such as the number of riders, number of drivers, location category, customer loyalty status, number of past rides, average ratings, time of booking, vehicle type, expected ride duration, and historical cost of the rides. These features will serve as the basis for developing the dynamic pricing model. You can download the data from [**here**](#https://statso.io/dynamic-pricing-case-study/).\n\n### Objectives\n1. Develop a predictive model that accurately estimates optimal fares for rides based on real-time market conditions and historical data.\n2. Investigate the impact of various factors such as demand patterns, supply availability, and customer attributes on ride fares.\n3. Implement a dynamic pricing strategy that adjusts fares in response to changing factors to maximize revenue and customer satisfaction.\n\n### Methodology\n1. **Data Collection and Preprocessing**: Gather comprehensive datasets and preprocess them by handling missing values, encoding categorical variables, and normalizing numerical features.\n2. **Feature Engineering**: Extract relevant features and potentially create new ones to enhance the predictive power of the model, considering factors such as demand patterns and supply availability.\n3. **Model Selection**: Explore various machine learning algorithms, including regression and ensemble methods, to identify the optimal model for dynamic pricing prediction.\n4. **Model Training and Evaluation**: Train the selected model on the preprocessed dataset and evaluate its performance using appropriate metrics such as mean absolute error and root mean squared error.\n5. **Dynamic Pricing Implementation**: Implement the trained model to dynamically adjust fares in real-time based on changing market conditions, demand patterns, and supply availability, ensuring optimal pricing for each ride.\n\n### Libraries","metadata":{}},{"cell_type":"code","source":"# Ignore Warnings\nimport warnings\n\n\nimport dill\n\n# Data Manipulation\nimport pandas as pd\nimport numpy as np\n\n# Imputation - RandomForest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Transformation\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\n\n# Feature Selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# Metrics\nimport math\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.metrics import r2_score\n\n# Regression Algorithms\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n%config InlineBackend.figure_format = 'svg'\nwarnings.filterwarnings('ignore')  # Disabling warning outputs","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:09:52.299693Z","iopub.execute_input":"2024-05-02T22:09:52.300345Z","iopub.status.idle":"2024-05-02T22:09:55.180358Z","shell.execute_reply.started":"2024-05-02T22:09:52.300311Z","shell.execute_reply":"2024-05-02T22:09:55.179247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Project Structure\n\n1. [**Loading Data**](#first): Load the dataset from a file or source into a DataFrame for analysis.\n2. [**Data Preprocessing**](#second): Handle any missing or erroneous data, perform data type conversions, and clean the dataset for further analysis.\n3. [**Exploratory Data Analysis (EDA)**](#third): Explore the dataset to gain insights into its distribution, relationships, and patterns. Visualize key features and relationships between variables.\n4. [**Feature Engineering**](#fifth): Create new features or transform existing ones to improve the predictive power of the model.\n5. [**Data Splitting**](#sixth): Split the training dataset into features (x) and the target variable (y) to prepare for model training.\n6. [**Model Training**](#seventh): Utilize machine learning algorithms to train a predictive model on the training data.\n7. [**Model Evaluation**](#eighth): Evaluate the trained model's performance using appropriate metrics and techniques.\n","metadata":{}},{"cell_type":"markdown","source":"# <a id='first'></a> Loading Data","metadata":{}},{"cell_type":"code","source":"# Path to the CSV data file\nTRAIN_DATAPATH = '/kaggle/input/dynamic-pricing-dataset/dynamic_pricing.csv' \n\n\n# Reading data from the CSV file into a DataFrame using the first column as the index\ndf = pd.read_csv(TRAIN_DATAPATH)  \n\n\n# Displaying the first few rows of the DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:10:52.026899Z","iopub.execute_input":"2024-05-02T22:10:52.027813Z","iopub.status.idle":"2024-05-02T22:10:52.07224Z","shell.execute_reply.started":"2024-05-02T22:10:52.027783Z","shell.execute_reply":"2024-05-02T22:10:52.071192Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize empty lists to store object and non-object columns\nobj = []\nints = []\n\n# Loop through DataFrame columns\nfor col in df.columns:\n    # Check if column data type is object\n    if df[col].dtype == 'object':\n        # If object, append column name, unique values count, and count of missing values to 'obj' list\n        obj.append((col, df[col].nunique(), df[col].isna().sum()))\n    else:\n        # If non-object, append column name, unique values count, and count of missing values to 'ints' list\n        ints.append((col, df[col].nunique(), df[col].isna().sum(), df[col].skew()))\n\n# Determine the maximum length of 'obj' and 'ints' lists\nmax_len = max(len(obj), len(ints))\n\n# Extend 'obj' and 'ints' lists with empty tuples to match the maximum length\nobj.extend([('', '', '')] * (max_len - len(obj)))\nints.extend([('', '', '', '')] * (max_len - len(ints)))\n\n# Create a dictionary with keys representing column categories and values representing lists of corresponding data\ndata = {\n    'Categorical_columns': [x[0] for x in obj],\n    'cat_cols_uniques': [x[1] for x in obj],\n    'cat_cols_missing': [x[2] for x in obj],\n    'Numeric_columns': [x[0] for x in ints],\n    'int_cols_uniques': [x[1] for x in ints],\n    'int_cols_missing': [x[2] for x in ints],\n    'int_cols_skew': [x[3] for x in ints]\n}\n\n# Convert the dictionary into a pandas DataFrame\npd.DataFrame(data)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:10:55.175374Z","iopub.execute_input":"2024-05-02T22:10:55.176453Z","iopub.status.idle":"2024-05-02T22:10:55.207882Z","shell.execute_reply.started":"2024-05-02T22:10:55.176414Z","shell.execute_reply":"2024-05-02T22:10:55.206933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the analysis of the dataset:\n\n- The dataset consists of 1000 entries with no missing values across all columns.\n- It comprises both categorical and numerical features:\n  - **Categorical Columns**:\n    1. **Location_Category**: Represents the category of the ride location.\n      - Unique Values: 3\n      \n    2. **Customer_Loyalty_Status**: Indicates the loyalty status of the customer.\n      - Unique Values: 3\n      \n    3. **Time_of_Booking**: Indicates the time at which the ride was booked.\n      - Unique Values: 4\n      \n    4. **Vehicle_Type**: Represents the type of vehicle used for the ride.\n      - Unique Values: 2\n      \n  - **Numerical Columns**:\n    1. **Number_of_Riders**: Indicates the number of riders for the ride.\n      - Unique Values: 81\n      \n    2. **Number_of_Drivers**: Indicates the number of drivers available for the ride.\n      - Unique Values: 79\n      \n    3. **Number_of_Past_Rides**: Represents the number of past rides taken by the customer.\n      - Unique Values: 101\n      \n    4. **Average_Ratings**: Represents the average ratings received by the ride service.\n      - Unique Values: 151\n      \n    5. **Expected_Ride_Duration**: Indicates the expected duration of the ride in minutes.\n      - Unique Values: 171\n      \n    6. **Historical_Cost_of_Ride**: Represents the historical cost of the ride.\n      - Unique Values: 1000\n\n- The dataset provides a solid foundation for developing a dynamic pricing model for ride-sharing services. By leveraging both categorical and numerical features, the model can predict optimal fares based on real-time market conditions and historical data.\n","metadata":{}},{"cell_type":"code","source":"# This line of code retrieves the shape of the DataFrame 'df'\nshape = df.shape\nprint(shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:15.317938Z","iopub.execute_input":"2024-05-02T22:11:15.318763Z","iopub.status.idle":"2024-05-02T22:11:15.323513Z","shell.execute_reply.started":"2024-05-02T22:11:15.318733Z","shell.execute_reply":"2024-05-02T22:11:15.322525Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id='second'></a> Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Displaying concise summary information about the DataFrame, including\n# data types, non-null values, and memory usage\ndf.info()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:20.25969Z","iopub.execute_input":"2024-05-02T22:11:20.260046Z","iopub.status.idle":"2024-05-02T22:11:20.276968Z","shell.execute_reply.started":"2024-05-02T22:11:20.260019Z","shell.execute_reply":"2024-05-02T22:11:20.275926Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying the data types of each column in the DataFrame\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:21.532756Z","iopub.execute_input":"2024-05-02T22:11:21.533093Z","iopub.status.idle":"2024-05-02T22:11:21.540633Z","shell.execute_reply.started":"2024-05-02T22:11:21.533068Z","shell.execute_reply":"2024-05-02T22:11:21.539643Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Counting the number of duplicated rows in the DataFrame and then counting\n# the frequency of those counts to summarize the distribution of duplicated rows\ndf.duplicated().value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:22.438567Z","iopub.execute_input":"2024-05-02T22:11:22.439464Z","iopub.status.idle":"2024-05-02T22:11:22.451344Z","shell.execute_reply.started":"2024-05-02T22:11:22.439431Z","shell.execute_reply":"2024-05-02T22:11:22.450283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:23.385942Z","iopub.execute_input":"2024-05-02T22:11:23.386781Z","iopub.status.idle":"2024-05-02T22:11:23.392779Z","shell.execute_reply.started":"2024-05-02T22:11:23.386752Z","shell.execute_reply":"2024-05-02T22:11:23.391759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id='third'></a> Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Generate descriptive statistics for all numerical columns\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:25.197352Z","iopub.execute_input":"2024-05-02T22:11:25.198058Z","iopub.status.idle":"2024-05-02T22:11:25.226456Z","shell.execute_reply.started":"2024-05-02T22:11:25.198029Z","shell.execute_reply":"2024-05-02T22:11:25.225471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate descriptive statistics for categorical columns\ndf.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:11:26.311395Z","iopub.execute_input":"2024-05-02T22:11:26.312245Z","iopub.status.idle":"2024-05-02T22:11:26.330197Z","shell.execute_reply.started":"2024-05-02T22:11:26.312213Z","shell.execute_reply":"2024-05-02T22:11:26.329006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualising Categorical and Numerical features distributions","metadata":{}},{"cell_type":"code","source":"# Select numerical columns from the DataFrame\nnumerics = df.select_dtypes(include='number')\n\n# Calculate the number of plots, rows, and columns for subplots\nnum_plots = len(numerics.columns)\nnum_columns = 3\nnum_rows = num_plots // num_columns + (1 if num_plots % num_columns > 0 else 0)\n\n\n\n# Set the figure size based on the number of rows\nplt.figure(figsize=(10, 4 * num_rows))\n\n# Iterate over each numerical column and create a histogram subplot\nfor i, col in enumerate(numerics, 1):\n    plt.subplot(num_rows, num_columns, i)  # Create subplot\n    mean_values = numerics[col].mean()\n    median = numerics[col].median()\n\n    sns.histplot(numerics[col], kde=True, color='#638889')  # Plot histogram using seaborn\n    plt.axvline(x=mean_values, color='#F28585', linestyle='--', label='Mean')\n    plt.axvline(x=median, color='#747264', linestyle='--', label='Median')\n    plt.grid(True, alpha=0.8)  # Add grid lines to the plot\n    plt.title(f'{col} Distribution')  # Set title for the subplot\n    plt.legend()\n\nplt.tight_layout()  # Adjust layout to prevent overlapping\nplt.show()  # Display the plots\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:12:48.188278Z","iopub.execute_input":"2024-05-02T22:12:48.188935Z","iopub.status.idle":"2024-05-02T22:12:50.809119Z","shell.execute_reply.started":"2024-05-02T22:12:48.188905Z","shell.execute_reply":"2024-05-02T22:12:50.807997Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select categorical columns from the DataFrame\ncategorical_cols = df.select_dtypes(include=['object']).columns\n\n# Calculate the number of plots, rows, and columns for subplots\nnum_plots = len(categorical_cols)\nnum_columns = 3\nnum_rows = num_plots // num_columns + (1 if num_plots % num_columns > 0 else 0)\n\n# Set the figure size based on the number of rows\nplt.figure(figsize=(10, 4 * num_rows))\n\n# Iterate over each categorical column and create a histogram subplot\nfor i, col in enumerate(df[categorical_cols], 1):\n    mode = df[col].mode()[0]    \n    plt.subplot(num_rows, num_columns, i)  # Create subplot\n    sns.histplot(df[col], kde=True, color='#638889')  # Plot histogram using seaborn\n\n    plt.axvline(x=mode, color='#F28585', linestyle='--', label='Mode')\n\n    plt.xticks(rotation=90, fontsize=7)  # Rotate x-axis labels for better readability\n    plt.title(f'{col} Distribution')  # Set title for the subplot\n\nplt.tight_layout()  # Adjust layout to prevent overlapping\nplt.show()  # Display the plots\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:14:19.768018Z","iopub.execute_input":"2024-05-02T22:14:19.768941Z","iopub.status.idle":"2024-05-02T22:14:21.154354Z","shell.execute_reply.started":"2024-05-02T22:14:19.768906Z","shell.execute_reply":"2024-05-02T22:14:21.153199Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outliers and Inconsistent Data","metadata":{}},{"cell_type":"markdown","source":"### Z-score Plot for Numerical Columns\n\nThis code snippet generates a series of subplots to visualize the distribution of Z-scores for each numerical column in the dataset. Z-score, also known as standard score, measures how many standard deviations an observation is from the mean. \n\n#### Steps:\n1. **Select Numerical Columns**: Only the numerical columns are selected from the dataset using `select_dtypes` function.\n\n2. **Calculate Plot Layout**: The number of subplots, rows, and columns are calculated based on the number of numerical columns.\n\n3. **Set Figure Size**: The size of the figure is adjusted based on the number of rows.\n\n4. **Iterate Through Numerical Columns**: For each numerical column, a subplot is created. Z-scores are calculated for each column and plotted against the index. A threshold line is drawn at Z-score of 3 and -3 to indicate outliers.\n\n#### Interpretation:\n- Points above the red dashed line (Z-score > 3) or below the red dashed line (Z-score < -3) are considered outliers.\n- This visualization helps in identifying potential outliers in the dataset for each numerical feature.\n\n","metadata":{}},{"cell_type":"code","source":"# Выберем только числовые столбцы\nnumerics = df.select_dtypes(include=np.number)\n\n# Calculate the number of plots, rows, and columns for subplots\nnum_plots = len(numerics.columns)\nnum_columns = 3\nnum_rows = num_plots // num_columns + (1 if num_plots % num_columns > 0 else 0)\n\n# Set the figure size based on the number of rows\nplt.figure(figsize=(10, 4 * num_rows))\n\nfor i, col in enumerate(numerics, 1):\n    plt.subplot(num_rows, num_columns, i)  \n    z_scores = (numerics[col] - numerics[col].mean()) / numerics[col].std()\n\n    threshold = 3\n\n    plt.scatter(np.arange(len(z_scores)), z_scores, color='#638889', alpha=0.5)\n    plt.axhline(y=threshold, color='#F28585', linestyle='--', label='Threshold')\n    plt.axhline(y=-threshold, color='#F28585', linestyle='--')\n    plt.xlabel('Index')\n    plt.ylabel('Z-score')\n    plt.title(f'Z-score Plot for {col}')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:15:21.88087Z","iopub.execute_input":"2024-05-02T22:15:21.881601Z","iopub.status.idle":"2024-05-02T22:15:24.169905Z","shell.execute_reply.started":"2024-05-02T22:15:21.881567Z","shell.execute_reply":"2024-05-02T22:15:24.169003Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualising correlation","metadata":{}},{"cell_type":"code","source":"# Set the figure size\nplt.figure(figsize=(8, 6))\n\n# Create a heatmap of the correlation matrix for numerical columns in the dataframe\nsns.heatmap(df.select_dtypes(include='number').corr(), annot=True, \n            cmap=['#638889', '#678788', '#6c8788', '#718788', '#768788',\n                  '#7b8788', '#808788', '#858788', '#8a8787', '#8f8787',\n                  '#948687', '#998687', '#9e8687', '#a38687', '#a88687',\n                  '#ac8686', '#b18686', '#b68686', '#bb8686', '#c08686',\n                  '#c58586', '#ca8586', '#cf8585', '#d48585', '#d98585',\n                  '#de8585', '#e38585', '#e88585', '#ed8585', '#f28585'], annot_kws={\"fontsize\":8})\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:15:54.249515Z","iopub.execute_input":"2024-05-02T22:15:54.250226Z","iopub.status.idle":"2024-05-02T22:15:54.906852Z","shell.execute_reply.started":"2024-05-02T22:15:54.250189Z","shell.execute_reply":"2024-05-02T22:15:54.905499Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a scatter plot with linear regression lines using seaborn's lmplot\nsns.lmplot(data=df, y='Historical_Cost_of_Ride', x='Expected_Ride_Duration', hue='Vehicle_Type', \n           palette=['#638889', '#f28585'])\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:15:55.166012Z","iopub.execute_input":"2024-05-02T22:15:55.166385Z","iopub.status.idle":"2024-05-02T22:15:56.00204Z","shell.execute_reply.started":"2024-05-02T22:15:55.166354Z","shell.execute_reply":"2024-05-02T22:15:56.000945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id='fifth'></a> Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Dynamic Pricing Calculation\n\nThis section of code calculates the dynamic pricing adjustments for rides based on demand and supply factors. Here's a breakdown of the process:\n\n1. **Demand Multiplier Calculation**:\n   - `high_demand_percentile` and `low_demand_percentile` are defined to set the percentiles for high and low demand, respectively.\n   - The `demand_multiplier` is calculated based on the percentile for high and low demand. For each ride, it compares the number of riders (`Number_of_Riders`) with the respective percentiles. If the number of riders is above the high demand percentile, it divides the number of riders by the high demand percentile value. If it's below the low demand percentile, it divides by the low demand percentile value.\n\n2. **Supply Multiplier Calculation**:\n   - Similar to demand, `high_supply_percentile` and `low_supply_percentile` are defined for setting the percentiles for high and low supply, respectively.\n   - The `supply_multiplier` is calculated based on the percentile for high and low supply. For each ride, it compares the number of drivers (`Number_of_Drivers`) with the respective percentiles. If the number of drivers is above the low supply percentile, it divides the high supply percentile value by the number of drivers. If it's below the low supply percentile, it divides the low supply percentile value by the number of drivers.\n\n3. **Price Adjustment Factors**:\n   - `demand_threshold_high`, `demand_threshold_low`, `supply_threshold_high`, and `supply_threshold_low` are defined to set the thresholds for high and low demand/supply.\n\n4. **Adjusted Ride Cost Calculation**:\n   - The `adjusted_ride_cost` is calculated based on the historical cost of the ride (`Historical_Cost_of_Ride`) and the dynamic pricing adjustments determined by the demand and supply multipliers. It takes the maximum of demand multiplier and demand threshold low, and the maximum of supply multiplier and supply threshold high, then multiplies these values with the historical cost of the ride.\n\nOverall, this process adjusts the ride cost dynamically based on demand and supply conditions to optimize pricing and maximize revenue.","metadata":{}},{"cell_type":"code","source":"\n# Calculate demand_multiplier based on percentile for high and low demand\nhigh_demand_percentile = 75\nlow_demand_percentile = 25\n\ndf['demand_multiplier'] = np.where(df['Number_of_Riders'] > np.percentile(df['Number_of_Riders'], high_demand_percentile),\n                                   df['Number_of_Riders'] / np.percentile(df['Number_of_Riders'], high_demand_percentile),\n                                   df['Number_of_Riders'] / np.percentile(df['Number_of_Riders'], low_demand_percentile))\n\n# Calculate supply_multiplier based on percentile for high and low supply\nhigh_supply_percentile = 75\nlow_supply_percentile = 25\n\ndf['supply_multiplier'] = np.where(df['Number_of_Drivers'] > np.percentile(df['Number_of_Drivers'], \n                                                                           low_supply_percentile),\n                                   np.percentile(df['Number_of_Drivers'], high_supply_percentile)\n                                   / df['Number_of_Drivers'], np.percentile(df['Number_of_Drivers'], \n                                                                            low_supply_percentile) \n                                   / df['Number_of_Drivers'])\n\n# Define price adjustment factors for high and low demand/supply\ndemand_threshold_high = 1.2  # Higher demand threshold\ndemand_threshold_low = 0.8  # Lower demand threshold\nsupply_threshold_high = 0.8  # Higher supply threshold\nsupply_threshold_low = 1.2  # Lower supply threshold\n\n# Calculate adjusted_ride_cost for dynamic pricing\ndf['adjusted_ride_cost'] = df['Historical_Cost_of_Ride'] * (\n    np.maximum(df['demand_multiplier'], demand_threshold_low) *\n    np.maximum(df['supply_multiplier'], supply_threshold_high)\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:17:33.040146Z","iopub.execute_input":"2024-05-02T22:17:33.040538Z","iopub.status.idle":"2024-05-02T22:17:33.057089Z","shell.execute_reply.started":"2024-05-02T22:17:33.040509Z","shell.execute_reply":"2024-05-02T22:17:33.055919Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:17:37.936405Z","iopub.execute_input":"2024-05-02T22:17:37.937185Z","iopub.status.idle":"2024-05-02T22:17:37.954435Z","shell.execute_reply.started":"2024-05-02T22:17:37.937155Z","shell.execute_reply":"2024-05-02T22:17:37.953353Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Profitability Analysis of Rides\n\nThis section of code analyzes the profitability of rides by comparing the adjusted ride cost (calculated using dynamic pricing) with the historical cost of the ride. Here's a breakdown of the process:\n\n1. **Profit Percentage Calculation**:\n   - The profit percentage for each ride is calculated using the formula:\n   \n     $\n     \\text{Profit Percentage} = \\left( \\frac{{\\text{Adjusted Ride Cost} - \\text{Historical Cost of Ride}}}{{\\text{Historical Cost of Ride}}} \\right) \\times 100\n     $\n   - This formula computes the percentage increase or decrease in the cost of the ride after applying dynamic pricing compared to its historical cost.\n\n2. **Identification of Profitable and Loss Rides**:\n   - Profitable rides are identified where the profit percentage is positive.\n   - Loss rides are identified where the profit percentage is negative.\n\n3. **Counting of Profitable and Loss Rides**:\n   - The count of profitable and loss rides is calculated.\n\n4. **Visualization using Donut Chart**:\n   - A donut chart is created to visualize the distribution of profitable and loss rides.\n   - Labels and values are defined for 'Profitable Rides' and 'Loss Rides'.\n\nOverall, this analysis provides insights into the impact of dynamic pricing on the profitability of rides, helping to understand the effectiveness of the pricing strategy in maximizing revenue.","metadata":{}},{"cell_type":"code","source":"# Calculate the profit percentage for each ride\ndf['profit_percentage'] = ((df['adjusted_ride_cost'] - df['Historical_Cost_of_Ride']) \n                           / df['Historical_Cost_of_Ride']) * 100\n# Identify profitable rides where profit percentage is positive\nprofitable_rides = df[df['profit_percentage'] > 0]\n\n# Identify loss rides where profit percentage is negative\nloss_rides = df[df['profit_percentage'] < 0]","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:18:15.893564Z","iopub.execute_input":"2024-05-02T22:18:15.894316Z","iopub.status.idle":"2024-05-02T22:18:15.903861Z","shell.execute_reply.started":"2024-05-02T22:18:15.894278Z","shell.execute_reply":"2024-05-02T22:18:15.902833Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the profit percentage for each ride\ndf['profit_percentage'] = ((df['adjusted_ride_cost'] - df['Historical_Cost_of_Ride']) \n                           / df['Historical_Cost_of_Ride']) * 100\n# Identify profitable rides where profit percentage is positive\nprofitable_rides = df[df['profit_percentage'] > 0]\n\n# Identify loss rides where profit percentage is negative\nloss_rides = df[df['profit_percentage'] < 0]\n\n# Calculate the count of profitable and loss rides\nprofitable_count = len(profitable_rides)\nloss_count = len(loss_rides)\n\n# Create a donut chart to show the distribution of profitable and loss rides\nlabels = ['Profitable Rides', 'Loss Rides']\nvalues = [profitable_count, loss_count]\n\nplt.figure(figsize=(7, 7))\n\n# Create a pie chart\nplt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140, \n        colors = ['#638889', '#f28585'], labeldistance = 1.1,\n                  pctdistance = 0.85, normalize=True\n)\n\n# Draw a circle in the center to create a ring\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.axis('equal')\nplt.title('Profitability of Rides (Dynamic Pricing vs. Historical Pricing)')\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:18:26.521871Z","iopub.execute_input":"2024-05-02T22:18:26.522226Z","iopub.status.idle":"2024-05-02T22:18:26.662501Z","shell.execute_reply.started":"2024-05-02T22:18:26.522201Z","shell.execute_reply":"2024-05-02T22:18:26.660765Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Log Transformation of Target Variable\n\nIn this code snippet, the target variable `adjusted_ride_cost` is transformed using the natural logarithm plus one transformation (`np.log1p`). This transformation is commonly used to deal with skewed distributions and heteroscedasticity in regression analysis.\n\n#### Log Transformation:\n- The `np.log1p` function applies the natural logarithm transformation plus one to each value of the target variable `adjusted_ride_cost`.\n- Adding one before taking the logarithm helps avoid errors when the original value is zero.\n\n#### Benefits of Log Transformation:\n- **Normalization**: Log transformation can help normalize the distribution of the target variable, making it more symmetric.\n- **Homoscedasticity**: It can stabilize the variance of the target variable, making it more homoscedastic.\n- **Linear Relationships**: Log-transformed variables often result in more linear relationships with predictor variables in regression models.\n\n#### Considerations:\n- Log transformation is suitable for variables with right-skewed or positively skewed distributions.\n- However, it may not be appropriate for variables with zero or negative values.","metadata":{}},{"cell_type":"code","source":"# Calculate the skewness of the target variable 'TARGET' and round the result to 2 decimal places\nprint(\"Skewness: \", round(df['adjusted_ride_cost'].skew(), 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:18:57.577397Z","iopub.execute_input":"2024-05-02T22:18:57.57778Z","iopub.status.idle":"2024-05-02T22:18:57.584476Z","shell.execute_reply.started":"2024-05-02T22:18:57.577751Z","shell.execute_reply":"2024-05-02T22:18:57.583347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a new figure with a specified size (8 inches width, 6 inches height)\nplt.figure(figsize=(8, 6))\n\n\n# Calculate the mean and median of the 'adjusted_ride_cost' column\nmean_values = df['adjusted_ride_cost'].mean()\nmedian = df['adjusted_ride_cost'].median()\n\n# Add vertical lines for mean and median to the plot\nplt.axvline(x=mean_values, color='#F28585', linestyle='--', label='Mean')\nplt.axvline(x=median, color='#747264', linestyle='--', label='Median')\n\n# Create a histogram plot with KDE (Kernel Density Estimation)\nsns.histplot(df['adjusted_ride_cost'], kde=True, color='#638889')\n\n# Add grid lines to the plot\nplt.grid(True)\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:15.858483Z","iopub.execute_input":"2024-05-02T22:19:15.859281Z","iopub.status.idle":"2024-05-02T22:19:16.222Z","shell.execute_reply.started":"2024-05-02T22:19:15.859252Z","shell.execute_reply":"2024-05-02T22:19:16.220989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the natural logarithm transformation plus 1 to the target variable 'TARGET'\ndf['adjusted_ride_cost'] = np.log1p(df['adjusted_ride_cost'])","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:21.583262Z","iopub.execute_input":"2024-05-02T22:19:21.583974Z","iopub.status.idle":"2024-05-02T22:19:21.589056Z","shell.execute_reply.started":"2024-05-02T22:19:21.583925Z","shell.execute_reply":"2024-05-02T22:19:21.587909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a new figure with a specified size (8 inches width, 6 inches height)\nplt.figure(figsize=(8, 6))\n\n# Calculate the mean and median of the 'adjusted_ride_cost' column\nmean_value = df['adjusted_ride_cost'].mean()\nmedian = df['adjusted_ride_cost'].median()\n\n# Plot vertical lines indicating mean and median on the histogram plot\nplt.axvline(x=mean_value, color='#F28585', linestyle='--', label='Mean')\nplt.axvline(x=median, color='#747264', linestyle='--', label='Median')\n\n# Create a histogram plot with KDE (Kernel Density Estimation)\nsns.histplot(df['adjusted_ride_cost'], kde=True, color='#638889')\n\n# Add grid lines to the plot\nplt.grid(True)\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:22.45177Z","iopub.execute_input":"2024-05-02T22:19:22.452597Z","iopub.status.idle":"2024-05-02T22:19:22.857306Z","shell.execute_reply.started":"2024-05-02T22:19:22.452564Z","shell.execute_reply":"2024-05-02T22:19:22.855772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the skewness of the transformed target variable 'TARGET' and round the result to 2 decimal places\nprint(\"Skewness: \", round(df['adjusted_ride_cost'].skew(), 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:26.813072Z","iopub.execute_input":"2024-05-02T22:19:26.813792Z","iopub.status.idle":"2024-05-02T22:19:26.819203Z","shell.execute_reply.started":"2024-05-02T22:19:26.813763Z","shell.execute_reply":"2024-05-02T22:19:26.818208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### Box-Cox Transformation for Skewed Features\n\nIn this code snippet, a Box-Cox transformation is applied to skewed features in the dataset to reduce skewness and make the distribution more symmetric. Box-Cox transformation is a power transformation technique that is particularly effective for dealing with skewed data.\n\n#### Steps:\n1. **Estimating Lambda Parameter**: The lambda parameter for the Box-Cox transformation is estimated using `boxcox_normmax` function. Adding 2 to each value helps avoid non-positive values, which are not supported by the transformation.\n2. **Applying Box-Cox Transformation**: The Box-Cox transformation is applied to the feature using the estimated lambda parameter, `lambda_est`.\n\n\n#### Benefits of Box-Cox Transformation:\n- **Skewness Reduction**: Box-Cox transformation helps reduce skewness in the distribution of the features.\n- **Normalization**: It can make the distribution more symmetric, which is beneficial for many statistical analyses and machine learning algorithms.\n\n#### Considerations:\n- Box-Cox transformation assumes that the data is continuous and positive.\n- It may not be suitable for features with zero or negative values.\n- Care should be taken when estimating the lambda parameter, as incorrect estimation can lead to improper transformation.","metadata":{}},{"cell_type":"code","source":"# Print the skewness of the transformed 'Number_of_Drivers' column\nprint(\"Skewness: \", round(df['Number_of_Drivers'].skew(), 2))","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:45.771638Z","iopub.execute_input":"2024-05-02T22:19:45.772501Z","iopub.status.idle":"2024-05-02T22:19:45.778036Z","shell.execute_reply.started":"2024-05-02T22:19:45.77247Z","shell.execute_reply":"2024-05-02T22:19:45.777093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Estimate the optimal Box-Cox transformation parameter lambda for 'Number_of_Drivers' column\nlambda_est = boxcox_normmax(df['Number_of_Drivers'] + 2, brack=(-1.5, 1.5))\n\n# Apply the Box-Cox transformation to the 'Number_of_Drivers' column using the estimated lambda\ndf['Number_of_Drivers'] = boxcox1p(df['Number_of_Drivers'], lambda_est)\n\n# Print the skewness of the transformed 'Number_of_Drivers' column\nprint(\"Skewness: \", round(df['Number_of_Drivers'].skew(), 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:46.009358Z","iopub.execute_input":"2024-05-02T22:19:46.009759Z","iopub.status.idle":"2024-05-02T22:19:46.041725Z","shell.execute_reply.started":"2024-05-02T22:19:46.00973Z","shell.execute_reply":"2024-05-02T22:19:46.040675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id='sixth'></a> Data Splitting","metadata":{}},{"cell_type":"code","source":"# Create the TRAIN subset by selecting rows where the target variable TARGET is not null\nTARGET = df['adjusted_ride_cost']\n\n# Create the TEST subset by selecting rows where the target variable TARGET is null\ndf = df.drop(columns=['adjusted_ride_cost'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:54.756582Z","iopub.execute_input":"2024-05-02T22:19:54.757128Z","iopub.status.idle":"2024-05-02T22:19:54.76468Z","shell.execute_reply.started":"2024-05-02T22:19:54.757091Z","shell.execute_reply":"2024-05-02T22:19:54.763164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of columns for each data type and display the counts\ndf.dtypes.value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:19:57.478834Z","iopub.execute_input":"2024-05-02T22:19:57.479839Z","iopub.status.idle":"2024-05-02T22:19:57.488502Z","shell.execute_reply.started":"2024-05-02T22:19:57.479803Z","shell.execute_reply":"2024-05-02T22:19:57.487285Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Polynomial Feature Engineering\n\nIn this section of code, polynomial feature engineering is performed to create a new feature based on the relationship between the number of riders and the number of drivers. Here's a detailed explanation of the process:\n\n1. **Mapping Vehicle Type**:\n   - The 'Vehicle_Type' column in the DataFrame is mapped to numerical values. 'Economy' is assigned 0, and 'Premium' is assigned 1.\n\n2. **Feature Selection**:\n   - The feature matrix \\( X \\) is constructed from the DataFrame, including features such as 'Number_of_Riders', 'Number_of_Drivers', 'Expected_Ride_Duration', and 'Vehicle_Type'.\n\n3. **Polynomial Fitting**:\n   - A second-degree polynomial is fitted to the relationship between the number of riders and the number of drivers using the `np.polyfit()` function.\n   - This step calculates the coefficients of the polynomial that best fits the data.\n\n4. **Creating Polynomial Feature**:\n   - A polynomial is created based on the coefficients obtained from the polynomial fitting using the `np.poly1d()` function.\n   - This polynomial represents the relationship between the number of riders and the number of drivers.\n\n5. **Calculating Interpolated Division**:\n   - Values of the polynomial are computed for a new feature, 'interpolated_division', which is calculated as the division of the number of riders by the number of drivers.\n   - This step effectively interpolates the division feature using the polynomial relationship.\n\n6. **Conversion to Series**:\n   - The interpolated division feature is converted back to a Series object and added to the DataFrame as a new column.\n\nThis process enhances the feature set by incorporating a derived feature that captures the complex relationship between the number of riders and the number of drivers, potentially improving the performance of machine learning models trained on this data.","metadata":{}},{"cell_type":"code","source":"df['Vehicle_Type'] = df['Vehicle_Type'].map({'Economy': 0, 'Premium': 1})\n\n# Assign the feature matrix X to the TRAIN DataFrame\nX = df[['Number_of_Riders', 'Number_of_Drivers', 'Expected_Ride_Duration', 'Vehicle_Type']]\n\n\n# Подгоняем полином к набору данных\ncoefficients = np.polyfit(X['Number_of_Riders'].values, X['Number_of_Drivers'].values, deg=2)\n\n# Создаем полином на основе коэффициентов\npoly = np.poly1d(coefficients)\n\n# Вычисляем значения полинома для нового признака\ndivision_feature = poly(X['Number_of_Riders'].values / X['Number_of_Drivers'].values)\n\n\n# Преобразуем результат обратно в Series\nX['interpolated_division'] = pd.Series(division_feature)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:20:15.815646Z","iopub.execute_input":"2024-05-02T22:20:15.816573Z","iopub.status.idle":"2024-05-02T22:20:15.828724Z","shell.execute_reply.started":"2024-05-02T22:20:15.81654Z","shell.execute_reply":"2024-05-02T22:20:15.827553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Standardizing Features with StandardScaler\n\nIn this section, we standardize the features in the dataset \\(X\\) using the StandardScaler class from scikit-learn.\n\n1. **StandardScaler Instance Creation:**  \n   An instance of StandardScaler named `scaler` is created using the StandardScaler class. This instance will be used to standardize the features.\n\n2. **Feature Scaling:**  \n   The `fit_transform` method of the `scaler` object is called with the feature matrix \\(X\\) as input. This method computes the mean and standard deviation of each feature in \\(X\\) and then standardizes the features by subtracting the mean and dividing by the standard deviation.","metadata":{}},{"cell_type":"code","source":"# Assign the target vector y to the non-null values of the TARGET variable\ny = TARGET\n\n# Create an instance of StandardScaler\nscaler = StandardScaler()\n\n# Scale the features in X using StandardScaler and fit_transform method\nX_scaled = scaler.fit_transform(X)\n\n# Save the scaler to a file\nwith open('/kaggle/working/scaler.pkl', 'wb') as file:\n    dill.dump(scaler, file)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:21:15.797727Z","iopub.execute_input":"2024-05-02T22:21:15.798403Z","iopub.status.idle":"2024-05-02T22:21:15.811109Z","shell.execute_reply.started":"2024-05-02T22:21:15.79837Z","shell.execute_reply":"2024-05-02T22:21:15.810027Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Split the scaled features and the target variable y into training and testing sets\n# The test set will be 30% of the total data, and the random_state ensures reproducibility\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:21:21.260729Z","iopub.execute_input":"2024-05-02T22:21:21.261618Z","iopub.status.idle":"2024-05-02T22:21:21.269513Z","shell.execute_reply.started":"2024-05-02T22:21:21.261582Z","shell.execute_reply":"2024-05-02T22:21:21.268356Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id='seventh'></a> Model Training","metadata":{}},{"cell_type":"markdown","source":"### Model Evaluation with Grid Search and Cross-Validation\n\nThis code snippet demonstrates the process of evaluating multiple machine learning models using grid search and cross-validation. The goal is to find the best-performing model based on the R-squared score and root mean squared error (RMSE) on a test set.\n\n#### Steps:\n1. **Initialization**: \n   - An empty list `results` is initialized to store the evaluation results of each model.\n   - Variables `best_model` and `best_r2` are initialized to track the best model and its R-squared score, respectively.\n\n2. **Model Evaluation Loop**:\n   - Iterate over each model in the `models` dictionary, which contains the names and instances of different machine learning models.\n   - Create a pipeline with the current model.\n   - Perform grid search with cross-validation using `GridSearchCV`. If hyperparameters are defined for the model in the `params` dictionary, grid search is performed with those hyperparameters; otherwise, default hyperparameters are used.\n   - Fit the grid search to the training data.\n   - Make predictions on the test set using the best model obtained from grid search.\n   - Calculate the R-squared score and RMSE on the test set.\n   - Update the `best_model` and `best_r2` variables if the current model performs better than the previous best model.\n   - Append the evaluation results (model name, R-squared score, RMSE score, and best parameters) to the `results` list.\n\n3. **Results DataFrame**:\n   - Create a DataFrame `results_df` from the `results` list containing the evaluation results for each model.\n\n#### Benefits:\n- **Automated Model Selection**: Grid search with cross-validation automates the process of hyperparameter tuning, making it easier to find the best-performing model.\n- **Comprehensive Evaluation**: The evaluation results include both R-squared score and RMSE, providing insights into model performance.\n\n#### Considerations:\n- **Computational Complexity**: Grid search with cross-validation can be computationally expensive, especially for large datasets and complex models.\n- **Overfitting**: Care should be taken to prevent overfitting by using appropriate regularization techniques and validation strategies.\n","metadata":{}},{"cell_type":"code","source":"# Building Pipeline\n# Dictionary containing regression models\nmodels = {\n    'Ridge': Ridge(),\n    'Lasso': Lasso(),\n    'ElasticNet': ElasticNet(),\n    'SVR': SVR(),\n    'RandomForestRegressor': RandomForestRegressor(),\n    'XGBRegressor': XGBRegressor(objective='reg:squarederror'),\n    'CatBoostRegressor': CatBoostRegressor(verbose=0)\n}\n\n# Dictionary containing hyperparameter grids for each model\nparams = {\n    'Ridge': {'model__alpha': [0.1, 1.0, 10.0]},\n    'Lasso': {'model__alpha': [0.001, 0.01, 0.1]},\n    'ElasticNet': {'model__alpha': [0.001, 0.01, 0.1], 'model__l1_ratio': [0.2, 0.5, 0.8]},\n    'SVR': {'model__C': [0.1, 1, 10], 'model__epsilon': [0.1, 0.2, 0.5], 'model__kernel': ['linear', 'rbf']},\n    'RandomForestRegressor': {'model__n_estimators': [10, 50, 100]},\n    'XGBRegressor': {'model__learning_rate': [0.1, 0.5], 'model__max_depth': [3, 5, 7]},\n    'CatBoostRegressor': {'model__learning_rate': [0.01, 0.1, 0.5], 'model__depth': [4, 6, 8]}\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:21:26.643706Z","iopub.execute_input":"2024-05-02T22:21:26.644591Z","iopub.status.idle":"2024-05-02T22:21:26.656144Z","shell.execute_reply.started":"2024-05-02T22:21:26.644547Z","shell.execute_reply":"2024-05-02T22:21:26.655193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to calculate the root mean squared error (RMSE)\ndef rmse(y_true, y_pred):\n    return math.sqrt(mean_squared_error(y_true, y_pred))\n\n# Create a custom scorer 'rmse_scorer' using make_scorer\n# 'greater_is_better=False' indicates that lower values of the scoring function are better\nrmse_scorer = make_scorer(rmse, greater_is_better=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:21:27.857075Z","iopub.execute_input":"2024-05-02T22:21:27.857431Z","iopub.status.idle":"2024-05-02T22:21:27.862359Z","shell.execute_reply.started":"2024-05-02T22:21:27.857404Z","shell.execute_reply":"2024-05-02T22:21:27.861367Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize an empty list to store the results\nresults = []\n\n# Initialize variables to track the best model and its R-squared score\nbest_model = None\nbest_r2 = float('-inf')\n\n# Iterate over each model in the models dictionary\nfor name, model in models.items():\n    # Create a pipeline with the model\n    pipeline = Pipeline([\n        ('model', model)\n    ])\n    \n    # Check if hyperparameters for the current model are defined in the params dictionary\n    if name in params:\n        # If hyperparameters are defined, perform grid search with cross-validation\n        grid_search = GridSearchCV(pipeline, params[name], cv=5, scoring=rmse_scorer)\n    else:\n        # If hyperparameters are not defined, perform grid search with default hyperparameters\n        grid_search = GridSearchCV(pipeline, {}, cv=5, scoring=rmse_scorer)\n      \n    # Fit the grid search to the training data\n    grid_search.fit(X_train, y_train)\n    \n    # Make predictions on the test set\n    y_pred = grid_search.predict(X_test)\n    \n    # Calculate R-squared score and RMSE on the test set\n    r2 = r2_score(np.expm1(y_test), np.expm1(y_pred))\n    rmse_val = rmse(np.expm1(y_test), np.expm1(y_pred))\n\n\n    # Update the best model and its R-squared score if the current model performs better\n    if r2 > best_r2:\n        best_r2 = r2\n        best_model = grid_search\n    \n    # Append the results to the results list\n    results.append({\n        'Model': name,\n        'R-Squared Score': r2,\n        'RMSE Score': rmse_val,\n        'Best Parameters': grid_search.best_params_\n    })\n\n# Create a DataFrame from the results list\nresults_df = pd.DataFrame(results)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:21:50.546318Z","iopub.execute_input":"2024-05-02T22:21:50.547133Z","iopub.status.idle":"2024-05-02T22:22:47.007668Z","shell.execute_reply.started":"2024-05-02T22:21:50.5471Z","shell.execute_reply":"2024-05-02T22:22:47.006806Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <a id='eighth'></a> Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"### Model Evaluation on Test Set\n\nThis code snippet demonstrates how to evaluate the best model selected during the grid search process on the test set. The evaluation metric used here is the R-squared (R2) score, which measures the proportion of the variance in the target variable that is predictable from the features.\n\n#### Steps:\n1. **Prediction**: \n   - Make predictions on the test set using the best model (`best_model.predict(X_test)`).\n\n2. **R-squared Score Calculation**:\n   - Calculate the R-squared (R2) score using the predicted values and true target values.\n   - R-squared (R2) score measures the goodness-of-fit of the model, indicating how well the model explains the variability in the target variable.\n\n#### Benefits:\n- **Model Generalization Assessment**: The R-squared (R2) score on the test set provides insights into how well the model generalizes to unseen data.\n- **Performance Communication**: Communicating the R-squared (R2) score allows stakeholders to understand the effectiveness of the predictive model.\n\n#### Considerations:\n- **Interpretation**: A higher R-squared (R2) score indicates better model performance, with values closer to 1 indicating a better fit. However, R-squared should be interpreted in the context of the problem domain and compared to alternative models.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Set the display option to show all the contents of DataFrame columns without truncation\npd.set_option('display.max_colwidth', None)\n\n# Set the display option to format float numbers with a precision of 5 decimal places\npd.set_option('display.float_format', '{:.5f}'.format)\n\n# Display the DataFrame results_df\nresults_df\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:22:47.009687Z","iopub.execute_input":"2024-05-02T22:22:47.010265Z","iopub.status.idle":"2024-05-02T22:22:47.024585Z","shell.execute_reply.started":"2024-05-02T22:22:47.010223Z","shell.execute_reply":"2024-05-02T22:22:47.023727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Make predictions on the test set using the best model\ny_pred = best_model.predict(X_test)\n\n# Calculate the R-squared (R2) score using the predictions and true target values\nr2 = r2_score(np.expm1(y_test), np.expm1(y_pred))\n\n# Print the R-squared (R2) score\nprint(f\"R-squared (R2): {r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:22:47.025542Z","iopub.execute_input":"2024-05-02T22:22:47.025806Z","iopub.status.idle":"2024-05-02T22:22:47.037238Z","shell.execute_reply.started":"2024-05-02T22:22:47.025783Z","shell.execute_reply":"2024-05-02T22:22:47.036207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Residual Plot for Model Evaluation\n\nThis section demonstrates the creation of a residual plot to evaluate the performance of a predictive model.\n\n1. **Calculate Residuals:**  \n   The residuals are calculated by subtracting the predicted target values from the true target values $ y_{\\text{test}} - y_{\\text{pred}} $. Since the target variable was log-transformed during training (`np.expm1`), it is first transformed back to its original scale before calculating the residuals.\n\n2. **Calculate Standard Deviation of Residuals:**  \n   The standard deviation $\\sigma$ of the residuals is calculated using the `np.std` function.\n\n3. **Create Hue List:**  \n   A list is created for the `hue` parameter, which indicates whether the absolute value of each residual is less than the standard deviation (\\(\\sigma\\)). This helps differentiate between residuals within one standard deviation of the mean and those outside it.\n\n4. **Create Residual Plot:**  \n   Using seaborn's `scatterplot` function, a scatter plot of residuals against predicted values is created. Residuals within one standard deviation of the mean are plotted in one color, while those outside it are plotted in another color. A horizontal dashed line is drawn at y=0 to represent the ideal case where residuals are centered around zero.\n","metadata":{}},{"cell_type":"code","source":"# Calculate residuals\nresiduals = np.expm1(y_test) - np.expm1(y_pred)\n\n# Calculate standard deviation of residuals\nsigma = np.std(residuals)\n\n# Create a list for hue parameter\nhue = list(map(lambda x: abs(x) < sigma, residuals))\n\n# Create a residual plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y_pred, y=residuals, hue=hue, palette = ['#f28585', '#638889'], s=100)\nplt.axhline(y=0, color='#A79277', linestyle='--', lw=1)\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:23:15.999142Z","iopub.execute_input":"2024-05-02T22:23:15.999951Z","iopub.status.idle":"2024-05-02T22:23:16.515168Z","shell.execute_reply.started":"2024-05-02T22:23:15.999915Z","shell.execute_reply":"2024-05-02T22:23:16.514229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# # Define your CatBoostRegressor\n# model = CatBoostRegressor()\n\n# # Define the parameter distribution\n# param_dist = {\n#     'depth': np.arange(3, 11, 1),\n#     'learning_rate': np.linspace(0.01, 0.1, 10),\n#     'iterations': np.arange(30, 501, 20),\n#     'l2_leaf_reg': np.arange(1, 6, 1),\n#     'subsample': np.linspace(0.8, 1, 5)\n# }\n\n# # Create a RandomizedSearchCV object\n# random_search = RandomizedSearchCV(\n#     estimator=model,\n#     param_distributions=param_dist,\n#     n_iter=150,  # Number of parameter settings that are sampled\n#     scoring='neg_mean_squared_error',\n#     cv=5,\n#     verbose=0,\n#     random_state=42,  # For reproducibility\n#     n_jobs=-1\n# )\n\n# # Fit the RandomizedSearchCV object to the data\n# random_search.fit(X_train, y_train)\n\n# # Get the best parameters\n# best_params = random_search.best_params_\n# print(\"Best parameters: \", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:28:58.381058Z","iopub.execute_input":"2024-05-02T22:28:58.382026Z","iopub.status.idle":"2024-05-02T22:28:58.387626Z","shell.execute_reply.started":"2024-05-02T22:28:58.381967Z","shell.execute_reply":"2024-05-02T22:28:58.386541Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Applying the Hyperparameters to the model and Visualizing the log Loss","metadata":{}},{"cell_type":"code","source":"# Create a CatBoostRegressor model with specified hyperparameters\nmodel = CatBoostRegressor(verbose=0, subsample=0.9, learning_rate=0.05000000000000001, l2_leaf_reg=2,\n                          iterations=350, depth=4)\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Extract feature importances from the trained model\nfeature_coefficients = pd.DataFrame(model.feature_importances_, index=X.columns,\n                                    columns=['Importance']).sort_values(by='Importance', ascending=False)\n\n# Make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Calculate the R-squared (R2) score using the predictions and true target values\nr2 = r2_score(np.expm1(y_test), np.expm1(y_pred))\n\n# Print the R-squared (R2) score\nprint(f\"R-squared (R2): {r2}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:28:58.388943Z","iopub.execute_input":"2024-05-02T22:28:58.389308Z","iopub.status.idle":"2024-05-02T22:28:58.594189Z","shell.execute_reply.started":"2024-05-02T22:28:58.389281Z","shell.execute_reply":"2024-05-02T22:28:58.593106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization of Validation RMSE Over Iterations\n\nThis section demonstrates the visualization of the Root Mean Squared Error (RMSE) values during the training process using the CatBoostRegressor model.\n\n1. **Model Fitting with Evaluation Set:**  \n   The model is fitted to the training data (`X_train`, `y_train`) with an evaluation set specified as (`X_test`, `y_test`). This allows the model to evaluate its performance on the validation set during training. The `use_best_model=True` parameter ensures that the best model based on validation performance is retained.\n\n2. **Get Log Loss Values:**  \n   After training, the RMSE values on the validation set for each iteration are obtained using the `get_evals_result()` method of the model. The RMSE values are extracted from the 'validation' dictionary.\n\n3. **Plot Validation RMSE Over Iterations:**  \n   Using matplotlib, a line plot is created to visualize the change in RMSE over iterations during training. The x-axis represents the number of iterations, while the y-axis represents the RMSE values. The plot helps in understanding how the model's performance on the validation set evolves over the training process.\n","metadata":{}},{"cell_type":"code","source":"# Fit the model to the training data and evaluate it on the test set\n# use_best_model=True ensures that the model uses the best iteration based on early stopping\nmodel.fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\n\n# Get the RMSE values during training from the evaluation set\ntrain_log_loss = model.get_evals_result()['validation']['RMSE']\n\n# Plot RMSE over iterations\nplt.figure(figsize=(6, 4))\nplt.plot(np.arange(1, len(train_log_loss) + 1), train_log_loss, label='Validation RMSE', color='#638889')\nplt.xlabel('Iterations')\nplt.ylabel('RMSE')\nplt.title('Validation RMSE Over Iterations')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:29:19.201906Z","iopub.execute_input":"2024-05-02T22:29:19.202308Z","iopub.status.idle":"2024-05-02T22:29:19.777196Z","shell.execute_reply.started":"2024-05-02T22:29:19.202279Z","shell.execute_reply":"2024-05-02T22:29:19.775058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing Feature Importances","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\n\n# Create a pie chart\nplt.pie(feature_coefficients['Importance'].values, labels=feature_coefficients.index, autopct='%1.1f%%', startangle=140, \n        colors = ['#638889', '#868788', '#aa8687', '#ce8586', '#f28585'], labeldistance = 1.1,\n                  pctdistance = 0.85, normalize=True\n)\n\n# Draw a circle in the center to create a ring\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.axis('equal')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:29:34.345537Z","iopub.execute_input":"2024-05-02T22:29:34.345904Z","iopub.status.idle":"2024-05-02T22:29:34.463463Z","shell.execute_reply.started":"2024-05-02T22:29:34.345874Z","shell.execute_reply":"2024-05-02T22:29:34.462372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediction and Generation Submission\n\n","metadata":{}},{"cell_type":"code","source":"# Function to map vehicle type to numeric value\ndef get_vehicle_type_numeric(vehicle_type):\n    # Define a mapping from vehicle type to numeric value\n    vehicle_type_mapping = {\n        \"Premium\": 1,\n        \"Economy\": 0\n    }\n    # Get the numeric value for the given vehicle type\n    vehicle_type_numeric = vehicle_type_mapping.get(vehicle_type)\n    return vehicle_type_numeric\n  \n# Predicting price using user input values\ndef predict_price(number_of_riders, number_of_drivers, Expected_Ride_Duration, vehicle_type):\n    # Convert vehicle type to numeric representation\n    vehicle_type_numeric = get_vehicle_type_numeric(vehicle_type)\n    # Raise error if vehicle type is invalid\n    if vehicle_type_numeric is None:\n        raise ValueError(\"Invalid vehicle type\")\n    \n    # Convert lists to numpy arrays for numerical computation\n    number_of_riders = np.array(number_of_riders)\n    number_of_drivers = np.array(number_of_drivers)\n    \n    # Fit a polynomial regression model to the data\n    coefficients = np.polyfit(number_of_riders.ravel(), number_of_drivers.ravel(), deg=2)\n    poly = np.poly1d(coefficients)\n    \n    # Calculate division feature\n    division_feature = poly(number_of_riders / number_of_drivers)\n    \n    # Create input data array for prediction\n    input_data = np.array([number_of_riders, number_of_drivers, Expected_Ride_Duration, vehicle_type_numeric, division_feature])\n    \n    # Reshape input data for compatibility with the model\n    input = pd.DataFrame(input_data.reshape(1, -1))\n    \n    # Scale input data using scaler object\n    scaled_input_data = scaler.transform(input)\n    \n    # Make price prediction using the model\n    predicted_price = model.predict(scaled_input_data)\n    return predicted_price\n\n# Example prediction using user input values\nuser_number_of_riders = 42\nuser_number_of_drivers = 31\nuser_vehicle_type = \"Premium\"\nExpected_Ride_Duration = 76\npredicted_price = predict_price(user_number_of_riders, user_number_of_drivers, Expected_Ride_Duration, user_vehicle_type)\n\n# Print the predicted price\nprint(\"Predicted price:\", np.expm1(predicted_price))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:29:46.290111Z","iopub.execute_input":"2024-05-02T22:29:46.290516Z","iopub.status.idle":"2024-05-02T22:29:46.306475Z","shell.execute_reply.started":"2024-05-02T22:29:46.290485Z","shell.execute_reply":"2024-05-02T22:29:46.305429Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save the model to a file\nwith open('/kaggle/working/best_model.pkl', 'wb') as file:\n    dill.dump(model, file)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:30:06.84225Z","iopub.execute_input":"2024-05-02T22:30:06.842622Z","iopub.status.idle":"2024-05-02T22:30:06.850982Z","shell.execute_reply.started":"2024-05-02T22:30:06.842595Z","shell.execute_reply":"2024-05-02T22:30:06.849887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:30:12.108787Z","iopub.execute_input":"2024-05-02T22:30:12.109571Z","iopub.status.idle":"2024-05-02T22:30:15.390467Z","shell.execute_reply.started":"2024-05-02T22:30:12.109541Z","shell.execute_reply":"2024-05-02T22:30:15.389018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 --version","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:30:18.692462Z","iopub.execute_input":"2024-05-02T22:30:18.693379Z","iopub.status.idle":"2024-05-02T22:30:19.684067Z","shell.execute_reply.started":"2024-05-02T22:30:18.693339Z","shell.execute_reply":"2024-05-02T22:30:19.68271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!which python3","metadata":{"execution":{"iopub.status.busy":"2024-05-02T22:30:21.721303Z","iopub.execute_input":"2024-05-02T22:30:21.721692Z","iopub.status.idle":"2024-05-02T22:30:22.739789Z","shell.execute_reply.started":"2024-05-02T22:30:21.721661Z","shell.execute_reply":"2024-05-02T22:30:22.738439Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}